{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "AID2 - Lyrics Analysis - Complete",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pelardillo/aid/blob/main/AID2_Lyrics_Analysis_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc8zwmXtcmb6"
      },
      "source": [
        "# Introducción\n",
        "La música (del griego mousikē o “arte de las musas”) es una manifestación artística y cultural, con múltiples finalidades en la sociedad así como en el desarrollo cognitivo de los seres humanos, aportando al desarrollo psicomotriz, el aprendizaje de lenguas así como potenciar la inteligencia emocional. En su definición más básica se la describe como una composición ordenada de sonidos y silencios, que conservan ritmo y una melodía. Como tal, la música es un componente fundamental en la construcción de la cultura e identidad de la humanidad. \n",
        "\n",
        "Las canciones a su vez contienen versos y coros (letra), que complementan la experiencia sensorial, y enriquecen el mensaje que se transmite al oyente entre los tonos musicales, las letras y cómo son interpretadas por quienes las cantan. \n",
        "\n",
        "En este marco se busca realizar un estudio analitico en las letras para modelar y aprender cómo los diferentes artistas de cada géneros componen canciones.\n",
        "\n",
        "## Objetivos\n",
        "El estudio busca modelar y clasificar letras de canciones por artista y género, y poder identificar letras aún no aprendidas según del contenido de las letras, y de esa forma inferir su artista y género.\n",
        "\n",
        "Para se propone aplicar un modelo de deep learning, aplicando NLP (Natural Language Processing) que permite analizar el contenido de las letras y aprender a reconocer los patrones de cada artista y género.\n",
        "\n",
        "El objetivo del modelo es poder clasificar una letra de una canción no antes vista (que no pertenezca al dataset de entrenamiento) mediante el análisis del contenido. La clasificación consiste en determinar el género de la canción.\n",
        "Una vez entrenado, el modelo deberá poder inferir el género en función de la letra, o fragmento de la misma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-95itGRnApk"
      },
      "source": [
        "### Hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-UmOPWgm9ds"
      },
      "source": [
        "# largo minimo en palabras de una cancion valida para el modelo.\n",
        "HP_MIN_SONG_LEN = 250\n",
        "\n",
        "# canciones por genero a cargar\n",
        "HP_SONGS_SAMPLES = 15000\n",
        "\n",
        "HP_MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "# tamaño del batch\n",
        "HP_BATCH_SIZE = 128\n",
        "\n",
        "# tamaño del embedding size\n",
        "HP_EMBEDDING_DIM = 100\n",
        "\n",
        "# filtros a utilizar por la CNN para procesar el texto\n",
        "HP_FILTERS = [2, 3, 4]\n",
        "\n",
        "# cantidad neuronas para cada filtro\n",
        "HP_N_FILTERS = 200\n",
        "\n",
        "# drop ratio\n",
        "HP_DROPOUT = 0.5\n",
        "\n",
        "# numero de epochs de training\n",
        "HP_N_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u45B7NtFPR9"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uDNT5DJeU11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228c1aa7-e52f-4e9a-ea3c-b4667eca0242"
      },
      "source": [
        "user = !whoami\n",
        "if user[0] == \"root\":\n",
        "  # enables googlw table plugin\n",
        "  %load_ext google.colab.data_table\n",
        "\n",
        "  # install API plugin\n",
        "  %pip install lyricsgenius\n",
        "  %pip install music_story\n",
        "\n",
        "  # !pip install torchtext --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lyricsgenius\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/c1/b7d56971a43e430214727daf774623d8edd0c13fe7bac1f484d0934af29b/lyricsgenius-2.0.2-py3-none-any.whl (46kB)\n",
            "\r\u001b[K     |███████▏                        | 10kB 31.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 20kB 35.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 30kB 20.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 40kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (2.10)\n",
            "Installing collected packages: lyricsgenius\n",
            "Successfully installed lyricsgenius-2.0.2\n",
            "Collecting music_story\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/5e/158a0dca477d6d6843ab198ebe03851a8eae811db3bc50aee0c28bd09d2c/music_story-0.1.tar.gz\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from music_story) (0.6.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from music_story) (3.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from music_story) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->music_story) (20.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->music_story) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->music_story) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->music_story) (1.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->music_story) (50.3.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->music_story) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->music_story) (8.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->music_story) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->music_story) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib->music_story) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib->music_story) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib->music_story) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib->music_story) (2.10)\n",
            "Building wheels for collected packages: music-story\n",
            "  Building wheel for music-story (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for music-story: filename=music_story-0.1-cp36-none-any.whl size=8980 sha256=45dd19b92c90d919662c8d985dbd5afc93af9ddcec55b20cc01f97d16a159863\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/a9/63/0c63554ce3214e6230c16acaf1b41f94509e2a37f242b91c16\n",
            "Successfully built music-story\n",
            "Installing collected packages: music-story\n",
            "Successfully installed music-story-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OMw0fT5Rq09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d82a780-00f3-4d80-a6e1-674cf028083c"
      },
      "source": [
        "# misc\n",
        "import time\n",
        "import requests\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# music APIs\n",
        "import lyricsgenius\n",
        "import music_story\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, Dataset, Example\n",
        "\n",
        "# prettytable\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# plots\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
        "\n",
        "# spacy/nlp\n",
        "import spacy \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"popular\")\n",
        "stops = stopwords.words(\"english\")\n",
        "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiYz3qnwEzIL"
      },
      "source": [
        "### Funciones Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9viWfmJ2EyYq"
      },
      "source": [
        "def elapsed_time(start_time):\n",
        "    elapsed_time = time.time() - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    elapsed_ms = elapsed_time - (elapsed_mins * 60) - elapsed_secs\n",
        "    print(f'\\nelapsed time: {elapsed_mins}m {elapsed_secs}s {elapsed_ms}ms')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWaX4yVXIw4-"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDOTDw3lE5G7"
      },
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "  \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "  def __init__(self, examples, fields, filter_pred=None):\n",
        "      \"\"\"\n",
        "      Create a dataset from a pandas dataframe of examples and Fields\n",
        "      Arguments:\n",
        "          examples pd.DataFrame: DataFrame of examples\n",
        "          fields {str: Field}: The Fields to use in this tuple. The\n",
        "              string is a field name, and the Field is the associated field.\n",
        "          filter_pred (callable or None): use only exanples for which\n",
        "              filter_pred(example) is true, or use all examples if None.\n",
        "              Default is None\n",
        "      \"\"\"\n",
        "      self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "      if filter_pred is not None:\n",
        "          self.examples = filter(filter_pred, self.examples)\n",
        "      self.fields = dict(fields)\n",
        "      # Unpack field tuples\n",
        "      for n, f in list(self.fields.items()):\n",
        "          if isinstance(n, tuple):\n",
        "              self.fields.update(zip(n, f))\n",
        "              del self.fields[n]\n",
        "\n",
        "class SeriesExample(Example):\n",
        "  \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "\n",
        "  @classmethod\n",
        "  def fromSeries(cls, data, fields):\n",
        "      return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "  @classmethod\n",
        "  def fromdict(cls, data, fields):\n",
        "      ex = cls()\n",
        "      \n",
        "      for key, field in fields.items():\n",
        "          \n",
        "          if key not in data:\n",
        "              raise ValueError(\"Specified key {} was not found in \"\n",
        "              \"the input data\".format(key))\n",
        "          if field is not None:\n",
        "              setattr(ex, key, field.preprocess(data[key]))\n",
        "          else:\n",
        "              setattr(ex, key, data[key])\n",
        "      return ex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swsWw64vTpSk"
      },
      "source": [
        "def print_batch(batch):\n",
        "\n",
        "  dat_dtype = { \n",
        "    'names'   : ('Lyric',   'Lyrics', 'Gender'), \n",
        "    'formats' : ('|S1000',  '|S1000', 'i')\n",
        "  }\n",
        "\n",
        "  dat = np.zeros(len(batch), dat_dtype)\n",
        "\n",
        "  x = PrettyTable(dat.dtype.names)\n",
        "  x.align['Lyric'] = 'r'\n",
        "  x.align['Gender'] = 'r'\n",
        "\n",
        "  lyric_ts = batch.Lyric.to(torch.device('cpu'))\n",
        "  gender_ts = batch.Genre.to(torch.device('cpu'))\n",
        "\n",
        "  lyrics_dat = []\n",
        "  lyrics = []\n",
        "\n",
        "  for l in lyric_ts.t():\n",
        "    lyrics_dat.append(', '.join([str(x) for x in l.numpy()]))\n",
        "    lyrics.append(' '.join([TEXT.vocab.itos[x] for x in l.numpy()]).encode('utf-8'))\n",
        "\n",
        "  dat['Lyric'] = lyrics_dat\n",
        "  dat['Lyrics'] = lyrics\n",
        "  dat['Artist'] = list(map(lambda x: x.encode('utf-8'), batch.Artist))\n",
        "  dat['SName'] = list(map(lambda x: x.encode('utf-8'), batch.SName))\n",
        "  dat['Gender'] = gender_ts \n",
        "\n",
        "  for row in dat:\n",
        "      x.add_row(row)\n",
        "\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-P13o2iFqUz"
      },
      "source": [
        "def calculate_tf_idf(data):\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "  tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(songs['Lyric'])\n",
        "\n",
        "  tfidf = tfidf_vectorizer_vectors.todense()\n",
        "  \n",
        "  # TFIDF of words not in the doc will be 0, so replace them with nan\n",
        "  tfidf[tfidf == 0] = np.nan\n",
        "  \n",
        "  # Use nanmean of numpy which will ignore nan while calculating the mean\n",
        "  means = np.nanmean(tfidf, axis=0)\n",
        "  \n",
        "  # convert it into a dictionary for later lookup\n",
        "  means = dict(zip(tfidf_vectorizer.get_feature_names(), means.tolist()[0]))\n",
        "\n",
        "  tfidf = tfidf_vectorizer_vectors.todense()\n",
        "  \n",
        "  # Argsort the full TFIDF dense vector\n",
        "  ordered = np.argsort(tfidf*-1)\n",
        "  words = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "  dff = pd.DataFrame({ 'word': words, 'tf-idf': means.values() })\n",
        "  dff = dff.sort_values('tf-idf', ascending=False)\n",
        "  \n",
        "  return means"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p8ylt96C6tj"
      },
      "source": [
        "# Data\n",
        "Para construir el dataset se utilizó el siguiente juego de [datos](https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres), el cual contiene un listado csv de canciones con título, letra, artista e idioma, y otro la informacion de dichos artistas.\n",
        "\n",
        "El autor del dataset original utilizo R para obtener la informacion del sitio https://www.vagalume.com.br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM9Wi5WRjvD4"
      },
      "source": [
        "### Download Data\n",
        "Ambos archivos CSV están disponibles en el siguiente link público en Goole Drive: https://drive.google.com/file/d/1d9s2_Y3d502iFvnR2-n_lP0GS7EbNplN/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBEguheSMFpz"
      },
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0OEbmcpPezT",
        "outputId": "49ba5ef8-2d60-40aa-92ec-2380712010f9"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "my_file = Path(\"artists-data.csv\")\n",
        "if not my_file.is_file():\n",
        "    print(\"downloading data...\")\n",
        "\n",
        "    # descarga el archivo zip desde google drive\n",
        "    download_file_from_google_drive('1d9s2_Y3d502iFvnR2-n_lP0GS7EbNplN', 'archive.zip')\n",
        "\n",
        "    # extraer y borrar el archivo zip\n",
        "    !unzip -o archive.zip\n",
        "    !rm archive.zip\n",
        "else:\n",
        "  print(\"data available\")\n",
        "\n",
        "# cargamos los archivos csv para ser procesados\n",
        "artists = pd.read_csv('artists-data.csv')\n",
        "lyrics = pd.read_csv('lyrics-data.csv')\n",
        "\n",
        "elapsed_time(start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading data...\n",
            "Archive:  archive.zip\n",
            "  inflating: artists-data.csv        \n",
            "  inflating: lyrics-data.csv         \n",
            "\n",
            "elapsed time: 0m 6s 0.4975142478942871ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JebehI2BkWHr"
      },
      "source": [
        "### Normalización de los Datos\n",
        "Es deseable procesar los datos para eliminar \n",
        "Los datos contenidos en el dataset contienen elementos que no contribuyen al significado y deben ser eliminados para mejorar la performance del modelo y reducir los tiempos de computo. Para ello utilizamos SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6onvxewYSv3L"
      },
      "source": [
        "# join de artistas y letras\n",
        "def get_songs(left, right, lng, gnrs):\n",
        "  s = pd.merge(left=left, right=right, how='inner', left_on='ALink', right_on='Link')\n",
        "  return s[(s.Idiom.isin(lng)) & (s.Genre.isin(gnrs))].drop_duplicates(subset=['SName'], keep='first').copy()\n",
        "\n",
        "# normaliza las letras utilizando SpaCy\n",
        "def normalize(comment, lowercase, remove_stopwords):\n",
        "  comment = re.sub(r\"[^a-zA-Z0-9]+\", ' ', comment)\n",
        "  if lowercase:\n",
        "      comment = comment.lower()\n",
        "  comment = nlp(comment)\n",
        "  lemmatized = list()\n",
        "  for word in comment:\n",
        "      lemma = word.lemma_.strip()\n",
        "      if lemma:\n",
        "          if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
        "              lemmatized.append(lemma)\n",
        "  return \" \".join(lemmatized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "ZfqklkwSP9_5",
        "outputId": "d4008217-e431-4a29-c5c3-163eb2c3fc6f"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# filtrar canciones en ingles\n",
        "songs = get_songs(lyrics, artists, ['ENGLISH'], ['Rock', 'Pop', 'Hip Hop'])\n",
        "print(f'all songs: {len(songs)}')\n",
        "\n",
        "# pre-procesar las canciones\n",
        "songs['Lyric'] = songs['Lyric'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
        "songs['Lyric'] = songs['Lyric'].apply(lambda x: re.sub(r'\\[[^\\]]*\\]', '', x))\n",
        "songs['Lyric'] = songs['Lyric'].apply(normalize, lowercase=True, remove_stopwords=False)\n",
        "\n",
        "songs['Len'] = songs['Lyric'].apply(lambda x: len(x.split(' ')))\n",
        "songs = songs[(songs['Len'] > HP_MIN_SONG_LEN) & (~songs['SName'].str.contains('tablatura'))];\n",
        "songs = songs.groupby('Genre').head(HP_SONGS_SAMPLES)\n",
        "\n",
        "print(f'filtered songs: {len(songs)}')\n",
        "\n",
        "# remove extra columns\n",
        "songs = songs[['Artist', 'SName', 'Lyric', 'Genre', 'Len']].copy()\n",
        "\n",
        "elapsed_time(start_time)\n",
        "\n",
        "songs.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all songs: 67226\n",
            "filtered songs: 31333\n",
            "\n",
            "elapsed time: 1m 12s 0.03745675086975098ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 6,\n            'f': \"6\",\n        },\n\"10000 Maniacs\",\n\"A Campfire Song\",\n\"a lie to say o my mountain have coal vein and bed to dig 500 man with axis and they all dig for me a lie to ssay o my river where mant fish do swim half of the catch be mine when you haul your net in never will he believe that his greed be a blind ray no devil or redeemer will cheat him he will take his gold to where he s lie cold a lie to say o my mine give a diamond a big a a fist but with every gem in his pocket the jewel he have miss a lie to say o my garden be grow tall by the day he only eat the well and toss the rest away never will he be believe that his greed be a blind ray no devil or redeemer can cheat him he will take his gold to where he s lie cold six deep in the grave something be out of reach something he want something be out of reach he s be taunt something be out of reach that he can beg or steal nor can he buy his old pain and fear in life there will not be time his old pain and fear in life there will not be time a lie to say o my forest have tree that block the sun and when i cut them down i don t answer to anyone no no never will he believe that his greed be a blind ray no devil or redeemer can cheat him he will take his gold where he s lie cold\",\n\"Rock\",\n{\n            'v': 273,\n            'f': \"273\",\n        }],\n [{\n            'v': 10,\n            'f': \"10\",\n        },\n\"10000 Maniacs\",\n\"Don't Talk\",\n\"don t talk i will listen don t talk you keep your distance for i have rather hear some truth tonight than entertain your lie so take you poison silently let me be let me close my eye don t talk i will believe it don t talk listen to me instead i know that if you think of it both long enough and hard the drink you drown your trouble in be the trouble you re in now talk talk talk about it if you talk a if you care but when your talk be over tilt that bottle in the air toss back much than your share don t talk i can guess it don t talk good now your restless and you need somewhere to put the blame for how you feel inside you will look for a close and easy mark and you will see me a fair game talk talk talk about it talk a if you care but when your talk be over tilt that bottle in the air toss back much than your share you talk talk talk about it you talk a if you care i be mark every word and can tell this time for sure your talk be the fine i have hear so don t talk i will be sleep let me go on dream how your eye they glow so fiercely i can tell your inspire by the name you just choose for me now what be it o never mind it we will talk talk talk about this when your head be clear i will discuss this in the morning but until then you may talk but i win t hear\",\n\"Rock\",\n{\n            'v': 284,\n            'f': \"284\",\n        }],\n [{\n            'v': 22,\n            'f': \"22\",\n        },\n\"10000 Maniacs\",\n\"Back O' The Moon\",\n\"jenny jenny you don t know the night i hide below a 2 story room to whistle you down the man who s let to divvy up time be a miser he s get a silver coin only let it shine for hour while you sleep it away there s one rare and odd style of live part only know to the everybody jenny a comical where s the end parade of the sort people here would think unusual jenny tonight upon the mock brine of a luna sea far off we sail on to back o the moon jenny jenny you don t know the day i have try tell backyard tale so to maybe amuse o your mood be never giddy if you smile i be delight but you have rather pout such a lazy child you dare fold your arm tisk and say that i lie there s one rare and odd style of think part only know to the everybody jenny the small step and giant leap taker get the head start in the race toward it jenny tonight upon the mock brine of a luna sea far off we sail on to the back o the moon that be a sigh but not mean to envy you when your age be mine some thing be swear true morning would come and calendar page have new print season on their opposite side jenny jenny you don t know the night i hide below a 2 story room to whistle you down o the man who s let to divvy up time be a miser he s get a silver coin let it shine for hour while you sleep it away there s one rare and odd style of live part only know to the everybody jenny out of tin ship jump the bubble head boy to push their flag into powder soil and cry no 2 placer no smart look goose in bonnet dance with pig in high button trouser no milk pail for the farmer s daughter no merry town of sweet wall house here i have find back o the moon not here i have find back o the moon\",\n\"Rock\",\n{\n            'v': 366,\n            'f': \"366\",\n        }],\n [{\n            'v': 26,\n            'f': \"26\",\n        },\n\"10000 Maniacs\",\n\"Like The Weather\",\n\"the color of the sky a far a i can see be coal grey lift my head from the pillow and then fall again with a shiver in my bone just think about the weather a quiver in my lip a if i may cry good by the force of will my lung be fill and so i breathe lately it seem this big bed be where i never leave shiver in my bone just think about the weather quiver in my voice a i cry what a cold and rainy day where on earth be the sun hide away i hear the sound of a noon bell chime now i be far behind you have put in bout half a day while here i lie with a shiver in my bone just think about the weather a quiver in my lip a if i may cry what a cold and rainy day where on earth be the sun hide away do i need someone here to scold me or do i need someone who will grab and pull me out of this four poster dull torpor pull downward for it be such a long time since my well day i say my prayer nightly this will pass away the color of the sky be grey a i can see through the blind lift my head from the pillow and then fall again with a shiver in my bone just think about the weather a quiver in my voice a i cry what a cold and rainy day where on earth be the sun hide away i shiver quiver and try to wake\",\n\"Rock\",\n{\n            'v': 273,\n            'f': \"273\",\n        }],\n [{\n            'v': 28,\n            'f': \"28\",\n        },\n\"10000 Maniacs\",\n\"Eat For Two\",\n\"oh baby blanket and baby shoe baby slipper baby spoon wall of baby blue dream child in my head be a nightmare bear in a borrow bed now i know lightning strike again it strike me once then strike me dead my folly grow inside of me i eat for two walk for two breathe for two now walk for two breathe for two eat for two now good the egg man fall down off his shelf all the good king s man with all their help struggle til the end for a shell they couldn t mend you know where this will lead to hush and rock in the nursery for the kick one inside of me i eat for two walk for two breathe for two now eat for two walk for two breathe for two now when the boy be a boy the girl be a girl they find each other in a wicked world strong in some respect but she couldn t stand for the way he beg and give in pride be for man young girl should run and hide instead you risk the game by take dare with yes i eat for two walk for two breathe for two now eat for two walk for two breathe for two now i walk for two i be stumble i walk for two i be stumble breathe for two i can t breathe i can t breathe five month how it grow five month now i begin to show\",\n\"Rock\",\n{\n            'v': 253,\n            'f': \"253\",\n        }],\n [{\n            'v': 30,\n            'f': \"30\",\n        },\n\"10000 Maniacs\",\n\"Maddox Table\",\n\"the leg of maddox kitchen table my whole life twist on a lathe in a foreman s torrent my \\ufeff1 english be fast boy if you want your pay bark command loud and simple we can all obey then i be forever pull silver rub the sawdust always deep in my eye varnish vapor that can linger on my skin it hold tight the whine of spin blade still echo to bother my sleep at night see that ox stamp dead center on the letter head of the company mail four decade a spit image of the animal i portray at maddox table a yoke be carve for my neck sun through the window oil spatter and in mason jar trick plenty seed thrive the stand joke around the shop be with my green thumb anything have grow my part be to laugh show and ornery jig have cut it at the knuckle bone see that ox trade mark burn into every stick of furniture from horn to tail four decade a spit image of the animal i portray at maddox table a yoke be carve for my neck be tailor make o my dolly be a weak not a burden girl treat her to a piece of vaudville a wintergarden move picture show bemus point on july sundays by trolley we have go to your benefit we s strike or bargain with the wave fist a union man not just for smoke spirit candy and cologne but for automobile key cash in the bank and the deed on a place call home\",\n\"Rock\",\n{\n            'v': 263,\n            'f': \"263\",\n        }],\n [{\n            'v': 38,\n            'f': \"38\",\n        },\n\"10000 Maniacs\",\n\"Poison In The Well\",\n\"tell me what s go wrong i tilt my head there under the faucet but when i turn it on dry a paper call the neighbor who s to blame for what s go on in the dark without a clue i be just the same a you o they tell us there s poison in the good that someone s be a bite untidy and there s be a small spill not a lot no just a drop but there you be mistake you know you be i wonder just how long they know our good be poison but they let us just drink on o they tell us there s poison in the good that someone s be a bite untidy and there s be a small spill all that it amount to be a tear in a salt sea someone s be a bite untidy they will have it clean up in a week but the week be over and now it s grow into year since i be tell that i should be calm there s nothing to fear here but i drink that water for year my wife and my child tell me where to now if your fight for a bearable life can be fight and lose in you backyard o don t tell us there s poison in the good that someone s be a bite untidy that there s be a small spill all that it amount to be a tear in a salt sea someone s be a bite untidy they will have it clean up in a week\",\n\"Rock\",\n{\n            'v': 268,\n            'f': \"268\",\n        }],\n [{\n            'v': 40,\n            'f': \"40\",\n        },\n\"10000 Maniacs\",\n\"Gold Rush Brides\",\n\"while the young folk be have their good time some of the mother be give birth to their baby three baby be bear in our company that summer my cousin emily give birth to a son in utah forty mile north of the great salt lake one morning but the next morning she travel on til noon when a stop be make and another child be bear this time susan mollmeyer and give the baby the name alice nevada follow the typical sign the hand paint line down prairie road pass the lone church spire pass the talk wire from where to who know there s no way to divide the beauty of the sky from the wild western plain where a man can drift in legendary myth by roam over space the land be free and the price be right dakota on the wall be a white robe woman broad yet maidenly such power in her hand a she hail the wagon man s family i see indians that crawl through this mural that recall our history who be the homestead wife who be the gold rush bride doe anybody know do their work survive their yellow fever live in the page they write the land be free yet it cost their live in miner s lust for gold a family s house be buy and sell piece by piece a widow stake her claim on a dollar and his name so painfully in letter mail back home her eastern sister they would moan a they would read account of madness childbirth loneliness and grief\",\n\"Rock\",\n{\n            'v': 266,\n            'f': \"266\",\n        }],\n [{\n            'v': 98,\n            'f': \"98\",\n        },\n\"10000 Maniacs\",\n\"Dreadlock Holiday\",\n\"i be walkin down the street concentratin on truckin right i hear a dark voice beside of me and i look round in a state of fright i see four face one mad a brother from the gutter they look me up and down a bite and turn to each other i say i don t like cricket oh no i love it i don t like cricket oh no i love it don t you walk thru my word you get to show some respect don t you walk thru my word cause you ain t hear me out yet good he look down at my silver chain he say i will give you one dollar i say you have get to be jokin man it be a present from me mother he say i like it i want it i will take it off your hand and you will be sorry you cross me you have well understand that you re alone a long way from home and i say i don t like reggae no no i love it i don t like reggae oh no i love it don t you cramp me style don t you queer on me pitch don t you walk thru my word cause you ain t hear me out yet i hurry back to the swim pool sinkin pina coladas i hear a dark voice beside me say would you like something hard she say i have get it you want it my harvest be the well and if you try it you will like it and wallow in a dreadlock holiday and i say don t like jamaica oh no i love her don t like jamaica oh no i love her oh yea don t you walk thru her word you get to show some respect don t you walk thru her word cause you ain t hear her out yet i don t like cricket i love it i don t like reggae i love it don t like jamaica i love her\",\n\"Rock\",\n{\n            'v': 346,\n            'f': \"346\",\n        }],\n [{\n            'v': 100,\n            'f': \"100\",\n        },\n\"10000 Maniacs\",\n\"Dust Bowl\",\n\"i should know to leave them home they follow me through the store with this toy i can t afford kid take them back you know well than that doll that talk astronaut t v game airplane they don t understand and how can i explain i try and try but i can t save penny nickel dollar slip away i have try and try but i can t save my young girl have bad fever sure all night with alcohol to cool and rub her down ruby i be tire try and get some sleep i be add doctor s fee to remedy with the cost of three day s work lose i try and try but i can t save penny nickel dollar slip away i have try and try but i can t save the hole in my pocketbook be grow there s a new wind blow they say it s go to be a cold cold one so brace yourself my darling it win t bring anything much our way but much dust bowl day i play a card in this week game take the \\ufeff1 and the last letter in three of their name this lottery s be build up for week i can be lucky me with the five million prize tear of disbelief spill out of my eye i try and try but i can t save penny nickel dollar slip away i have try and try but i can t save the hole in my pocketbook be grow there s a new wind blow they say it s go to be a cold cold one so brace yourself my darling it win t bring anything much our way but much dust bowl day\",\n\"Rock\",\n{\n            'v': 291,\n            'f': \"291\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Artist\"], [\"string\", \"SName\"], [\"string\", \"Lyric\"], [\"string\", \"Genre\"], [\"number\", \"Len\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Artist</th>\n",
              "      <th>SName</th>\n",
              "      <th>Lyric</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>A Campfire Song</td>\n",
              "      <td>a lie to say o my mountain have coal vein and ...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Don't Talk</td>\n",
              "      <td>don t talk i will listen don t talk you keep y...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Back O' The Moon</td>\n",
              "      <td>jenny jenny you don t know the night i hide be...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Like The Weather</td>\n",
              "      <td>the color of the sky a far a i can see be coal...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Eat For Two</td>\n",
              "      <td>oh baby blanket and baby shoe baby slipper bab...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Maddox Table</td>\n",
              "      <td>the leg of maddox kitchen table my whole life ...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Poison In The Well</td>\n",
              "      <td>tell me what s go wrong i tilt my head there u...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Gold Rush Brides</td>\n",
              "      <td>while the young folk be have their good time s...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Dreadlock Holiday</td>\n",
              "      <td>i be walkin down the street concentratin on tr...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>10000 Maniacs</td>\n",
              "      <td>Dust Bowl</td>\n",
              "      <td>i should know to leave them home they follow m...</td>\n",
              "      <td>Rock</td>\n",
              "      <td>291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Artist               SName  ... Genre  Len\n",
              "6    10000 Maniacs     A Campfire Song  ...  Rock  273\n",
              "10   10000 Maniacs          Don't Talk  ...  Rock  284\n",
              "22   10000 Maniacs    Back O' The Moon  ...  Rock  366\n",
              "26   10000 Maniacs    Like The Weather  ...  Rock  273\n",
              "28   10000 Maniacs         Eat For Two  ...  Rock  253\n",
              "30   10000 Maniacs        Maddox Table  ...  Rock  263\n",
              "38   10000 Maniacs  Poison In The Well  ...  Rock  268\n",
              "40   10000 Maniacs    Gold Rush Brides  ...  Rock  266\n",
              "98   10000 Maniacs   Dreadlock Holiday  ...  Rock  346\n",
              "100  10000 Maniacs           Dust Bowl  ...  Rock  291\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsHaNXNpIoXw"
      },
      "source": [
        "### Analisis de los Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbzOugeGS0WS"
      },
      "source": [
        "# print(songs.groupby('Genre').describe())\n",
        "# songs.groupby('Genre').describe().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHJd3dxR5K1E"
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# def draw_plot(genre, genre_songs):\n",
        "#   tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "#   tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(genre_songs['Lyric'])\n",
        "\n",
        "#   tfidf = tfidf_vectorizer_vectors.todense()\n",
        "\n",
        "#   # TFIDF of words not in the doc will be 0, so replace them with nan\n",
        "#   tfidf[tfidf == 0] = np.nan\n",
        "\n",
        "#   # Use nanmean of numpy which will ignore nan while calculating the mean\n",
        "#   means = np.nanmean(tfidf, axis=0)\n",
        "\n",
        "#   # convert it into a dictionary for later lookup\n",
        "#   means = dict(zip(tfidf_vectorizer.get_feature_names(), means.tolist()[0]))\n",
        "\n",
        "#   tfidf = tfidf_vectorizer_vectors.todense()\n",
        "\n",
        "#   # Argsort the full TFIDF dense vector\n",
        "#   ordered = np.argsort(tfidf*-1)\n",
        "#   words = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "#   dff = pd.DataFrame({ 'word': words, 'tf-idf': means.values() })\n",
        "#   dff = dff.sort_values('tf-idf', ascending=True)\n",
        "#   dff[dff['tf-idf'] >= 0.6].head(10).plot('word', 'tf-idf', 'bar', title=genre)\n",
        "\n",
        "#   return dff\n",
        "\n",
        "# # plotear words count global vs por genero\n",
        "# start_time = time.time()\n",
        "# draw_plot('@all', songs)\n",
        "# for genre, genre_songs in songs.groupby('Genre'):\n",
        "#   draw_plot(genre, genre_songs)\n",
        "# elapsed_time(start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-PZacT_NA5E"
      },
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# def plot(genre, genre_songs, color, all=None):\n",
        "#   count = Counter()\n",
        "#   for song in genre_songs['Lyric']:\n",
        "#     count += Counter(song.split())\n",
        "\n",
        "#   if all != None:\n",
        "#     for w, c in all.items():\n",
        "#       if count[w] < c - count[w]:\n",
        "#         count[w] = 0\n",
        "\n",
        "#   plt.bar(*zip(*count.most_common(10)), width=.5, color=color)\n",
        "#   plt.title(genre)\n",
        "#   plt.xlabel('word')\n",
        "#   plt.ylabel('freq')\n",
        "#   plt.show()\n",
        "\n",
        "#   return count\n",
        "\n",
        "# a = plot('@all', songs, 'b')\n",
        "# for g, s in songs.groupby('Genre'):\n",
        "#   plot(g, s, 'g', all=a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqCRumjFoymK"
      },
      "source": [
        "# Build Datasets\n",
        "Construimos los datasets de train, test y validation con las canciones procesesadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb2zJJowJsbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f9574a-b7c2-4ad6-f0f4-0cf95e3fd025"
      },
      "source": [
        "SEED = random.random()\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField()\n",
        "# SNAME = data.RawField(is_target=False)\n",
        "\n",
        "dataset = DataFrameDataset(songs, { \n",
        "                              'Lyric': TEXT\n",
        "                            , 'Genre': LABEL\n",
        "                            #, 'SName': SNAME \n",
        "})\n",
        "\n",
        "train_data, valid_data = dataset.split(split_ratio=0.7, random_state = random.seed(SEED))\n",
        "train_data, test_data = train_data.split(random_state = random.seed(SEED))\n",
        "\n",
        "elapsed_time(start_time)\n",
        "\n",
        "print(f'Number of all examples: {len(dataset)}')\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(test_data)}')\n",
        "print(f'Number of testing examples: {len(valid_data)}')\n",
        "\n",
        "assert len(train_data) + len(valid_data) + len(test_data) == len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "elapsed time: 0m 25s 0.6894738674163818ms\n",
            "Number of all examples: 31333\n",
            "Number of training examples: 15353\n",
            "Number of validation examples: 6580\n",
            "Number of testing examples: 9400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTK3QrEzKH67"
      },
      "source": [
        "### Build Vocab\n",
        "El objetivo de este paso es constuir el vocabulario del training_dataset y poder calcular los word embedding correspondientes.  \n",
        "\n",
        "Para entrenar el modelo se utilizaran pesos pre-entrenados ([GloVe](https://nlp.stanford.edu/projects/glove/)) para mejorar los tiempos, ya que se cuenta con una representación inicial de pesos para palabras y términos comunes ya entrenados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye5rlP_RKHM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edf3942-a6f4-422e-b88b-eb38ff6f8b55"
      },
      "source": [
        "MAX_VOCAB_SIZE = HP_MAX_VOCAB_SIZE\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# descarga pesos pre entrenados de GloVe u otros.\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
        "\n",
        "# crear vocab del label (genero)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "elapsed_time(start_time)\n",
        "\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)} {TEXT.vocab.freqs.most_common(10)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)} {LABEL.vocab.itos}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
            "100%|█████████▉| 398962/400000 [00:15<00:00, 25554.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "elapsed time: 7m 17s 0.14974021911621094ms\n",
            "Unique tokens in TEXT vocabulary: 25002 [('i', 305360), ('you', 247094), ('the', 223369), ('be', 206749), ('to', 145967), ('a', 138827), ('and', 124245), ('it', 123205), ('me', 100122), ('t', 94206)]\n",
            "Unique tokens in LABEL vocabulary: 3 ['Rock', 'Pop', 'Hip Hop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWPahdAL61va"
      },
      "source": [
        "Se construyen los batches en la gpu de estar disponible.\n",
        "Como las canciones no tienen un largo definido, pueden exitisr canciones con menos palabras que el largo del embedding (`HP_EMBEDDING_DIM`) para eso buscamos que las palabras queden agrupadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlQW1ixMo38E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7e8cf2-927b-4676-8b22-02c4a31be227"
      },
      "source": [
        "BATCH_SIZE = HP_BATCH_SIZE\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    #sort = True, # ordena los batches para procesar canciones de similar largo juntas\n",
        "    # sort_within_batch  = True,  # ordena los elementos dentro del batch\n",
        "    sort_key = lambda x: len(x.Lyric), # ordena los elementos del batch segun largo\n",
        "    device = device)\n",
        "\n",
        "# for batch in train_iterator:\n",
        "#     print_batch(batch)\n",
        "\n",
        "# for batch in valid_iterator:\n",
        "#     print_batch(batch)\n",
        "\n",
        "# for batch in test_iterator:\n",
        "#     print_batch(batch)    \n",
        "\n",
        "print(f'train batches: {len(train_iterator)}')\n",
        "    \n",
        "elapsed_time(start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train batches: 120\n",
            "\n",
            "elapsed time: 0m 0s 0.00034809112548828125ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1TnCK0fvkki"
      },
      "source": [
        "# Build Model\n",
        "El modelo utilizará diferentes filtros (2x2, 3x3, 4x4) para computar diferentes n-gramas. Luego se concatenan y se pasan a una capa densa. La arquitectura está basada en la publicación [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) por Yoon Kim. Como variante y desafío, en este caso se busca tener una clasificación múltiple de artista y género.\n",
        "\n",
        "Para esto la red se compone de las siguientes layers:\n",
        "1.   **Embeddings**\n",
        "2.   **Convolutional**\n",
        "3.   **Max Pool**\n",
        "4.   **Dropout**\n",
        "5.   **Dense**\n",
        "6.   **Softmax**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdwksKaMvkk0"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # se crean las N conv layers por cada tipo de filtro: 2, y 4 x N, siendo N el largo del embedding\n",
        "        self.convs = nn.ModuleList([\n",
        "          nn.Conv2d(in_channels = 1, out_channels = n_filters, kernel_size = (fs, embedding_dim)) for fs in filter_sizes\n",
        "        ])\n",
        "        \n",
        "        # capa densa/FC\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        # regularization por dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        text = text.permute(1, 0)\n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        # aplicamos las N convoluciones en paralelo, 1 por cada filtro definido\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        \n",
        "        # maxpool de cada convolucion y se guarda en un array\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        # aplicamos dropout a la concatenacion de todos los pools y generamos un vector de 1 x N x flter size\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        # retornamos el reultado de la capa densa/FC    \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awLPQkMCvkk3"
      },
      "source": [
        "### Crear Modelo\n",
        "\n",
        "Creamos la instancia del modelo con los parametros e hiperparametros corresponidnetes. \n",
        "`INPUT_DIM` es la dimension del vocab. \n",
        "`EMBEDDING_DIM` es el largo del word embedding, el largo del vector que contiene los tokens (tensores) de cada palabra generado a partir del input (full one-hot vector). \n",
        "`OUTPUT_DIM` corresponde a la dimension de la salida, la cual es igual al numero de clases $C$ que queremos aprender del modelo (`LABEL.vocab`). En este caso la cantidad de generos musicales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2oU5nvfvkk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b8c91b-b0de-469c-dc58-9ef14862ed3c"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = HP_EMBEDDING_DIM\n",
        "N_FILTERS = HP_N_FILTERS\n",
        "FILTER_SIZES = HP_FILTERS\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = HP_DROPOUT\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # <pad>\n",
        "\n",
        "print('CNN(', INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX, ')\\n')\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "for layer in model.children():\n",
        "  num_params = sum(p.numel() for p in layer.parameters())\n",
        "  print(f\"Layer: {layer}, Parameters: {num_params}\") \n",
        "\n",
        "print(f'\\nThe model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN( 25002 100 200 [2, 3, 4] 3 0.5 1 )\n",
            "\n",
            "Layer: Embedding(25002, 100), Parameters: 2500200\n",
            "Layer: ModuleList(\n",
            "  (0): Conv2d(1, 200, kernel_size=(2, 100), stride=(1, 1))\n",
            "  (1): Conv2d(1, 200, kernel_size=(3, 100), stride=(1, 1))\n",
            "  (2): Conv2d(1, 200, kernel_size=(4, 100), stride=(1, 1))\n",
            "), Parameters: 180600\n",
            "Layer: Linear(in_features=600, out_features=3, bias=True), Parameters: 1803\n",
            "Layer: Dropout(p=0.5, inplace=False), Parameters: 0\n",
            "\n",
            "The model has 2,682,603 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEmtDc_DInRW"
      },
      "source": [
        "### Adjust Embeddings\n",
        "Ajustamos los pesos del modelo de embeding con los pre-trainned descargados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUygtgPQvkk8"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "if pretrained_embeddings != None:\n",
        "  model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss9Vz2Rzvkk-"
      },
      "source": [
        "Dejamos en 0 los pesos de `UNK` (unknown word) y `PAD` (representa los espacios cuando las palabras no tiene el largo del embedding `HP_EMBEDDING_DIM`) tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-iilkvvkk-"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij29oNzJFCMi"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLPxc-S5JMAi"
      },
      "source": [
        "## Fitting Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLaAIkMu_4KN"
      },
      "source": [
        "El modelo produce como resultado un vector de dimension $C$ (y_hat), siendo c el numero de clases (generos) y los valores en cada posicion la probablidad que tiene el input de pertencer a esas clases.\n",
        "\n",
        "Por ejempl, en nuestro caso: 'rock' = 0, 'hip hop' = 1 y 'pop' = 2. La salida del modelo seria algo del estilo: [4.2, 1.1, 0.5]\n",
        "\n",
        "Calculamos la accuracy del modelo onteniendo `argmax` de ese vector, lo que nos devuelve el indice del elemento del array con mayor valor (en este caso 0), y comparamos eso con la label / Y para determinar si coinciden. Calculamos el accuracy de todo el batch. Obtener 8 correctas de 10 en un mismo batch implica un retorno de 0.8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNs3MhSg9yxq"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgHQdcWFvklF"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  \n",
        "  model.train()\n",
        "  \n",
        "  for batch in iterator:\n",
        "      \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    predictions = model(batch.Lyric)\n",
        "    \n",
        "    loss = criterion(predictions, batch.Genre)\n",
        "    \n",
        "    acc = categorical_accuracy(predictions, batch.Genre)\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "      \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG8Z0fbjvklH"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "    \n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "  \n",
        "    for batch in iterator:\n",
        "\n",
        "      predictions = model(batch.Lyric)\n",
        "            \n",
        "      loss = criterion(predictions, batch.Genre)\n",
        "        \n",
        "      acc = categorical_accuracy(predictions, batch.Genre)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "    \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEAg7hQ2vklL"
      },
      "source": [
        "## Run Train (WIP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3W9Zym4wOQ"
      },
      "source": [
        "Se utiliza [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) como algoritmo de optimizacion y [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) como loss function para el calculo de multi-class classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_U4_h_Yq48w"
      },
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LEZ-pcL4vklL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c72f40-61e9-456e-9d71-aeb0c20102c2"
      },
      "source": [
        "# creamos el optimizer, en este caso Adam\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# creamos la función de loss, en este cao BCE para multiple class\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# enviamos a la gpu el modelo\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "# numero de epochs\n",
        "N_EPOCHS = HP_N_EPOCHS\n",
        "\n",
        "# el \"mejor\" loss\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'lyrics-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "training_epoch_mins, training_epoch_secs = epoch_time(training_start_time, time.time())\n",
        "print(f'Training Duration: {training_epoch_mins}m {training_epoch_secs}s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 398962/400000 [00:29<00:00, 25554.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.840 | Train Acc: 59.47%\n",
            "\t Val. Loss: 0.705 |  Val. Acc: 68.73%\n",
            "Epoch: 02 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.695 | Train Acc: 68.29%\n",
            "\t Val. Loss: 0.661 |  Val. Acc: 70.77%\n",
            "Epoch: 03 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.617 | Train Acc: 73.08%\n",
            "\t Val. Loss: 0.654 |  Val. Acc: 71.19%\n",
            "Epoch: 04 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.552 | Train Acc: 76.41%\n",
            "\t Val. Loss: 0.634 |  Val. Acc: 72.27%\n",
            "Epoch: 05 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.483 | Train Acc: 79.99%\n",
            "\t Val. Loss: 0.640 |  Val. Acc: 72.31%\n",
            "Epoch: 06 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.416 | Train Acc: 83.81%\n",
            "\t Val. Loss: 0.656 |  Val. Acc: 72.25%\n",
            "Epoch: 07 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.338 | Train Acc: 87.56%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 72.84%\n",
            "Epoch: 08 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.281 | Train Acc: 89.95%\n",
            "\t Val. Loss: 0.703 |  Val. Acc: 72.26%\n",
            "Epoch: 09 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.228 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.751 |  Val. Acc: 72.68%\n",
            "Epoch: 10 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.190 | Train Acc: 93.51%\n",
            "\t Val. Loss: 0.797 |  Val. Acc: 72.06%\n",
            "Training Duration: 3m 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMhO3SpXvklN"
      },
      "source": [
        "Finally, let's run our model on the test set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk3xGRZ6FHo_"
      },
      "source": [
        "# Test Model (WIP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi3vvblpvklN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb567f4-ef48-4ec5-e78e-7b4c6504add1"
      },
      "source": [
        "# cargamos el modelo para test\n",
        "model.load_state_dict(torch.load('lyrics-model.pt'))\n",
        "\n",
        "# evaluamos el dataset de test\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.631 | Test Acc: 71.57%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyBmWCrwvklP"
      },
      "source": [
        "Similar to how we made a function to predict sentiment for any given sentences, we can now make a function that will predict the class of question given.\n",
        "\n",
        "The only difference here is that instead of using a sigmoid function to squash the input between 0 and 1, we use the `argmax` to get the highest predicted class index. We then use this index with the label vocab to get the human readable label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzuFZRdFvklQ"
      },
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load('en')\n",
        "def predict_class(model, sentence, min_len = 4):\n",
        "  model.eval()\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "  if len(tokenized) < min_len:\n",
        "    tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  preds = model(tensor)\n",
        "  max_preds = preds.argmax(dim = 1)\n",
        "  return max_preds.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6wGANb3vklR"
      },
      "source": [
        "Ejemplo aleatorio del test_set. Estos datos no fueron vistos por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChYEAexTyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cafad9e-3975-48c7-872e-37f18b33519d"
      },
      "source": [
        "corrects = 0\n",
        "\n",
        "for i in range(10):\n",
        "  random_song = test_iterator.dataset.examples[random.randint(0, len(test_iterator.dataset.examples) - 1)]\n",
        "\n",
        "  # input_title = random_song.SName\n",
        "  input_text = ' '.join(random_song.Lyric).strip()\n",
        "  input_label = random_song.Genre\n",
        "\n",
        "  print(f'\\ninput: ({input_label}) => {input_text[:50]}...')\n",
        "\n",
        "  pred_class = predict_class(model, input_text)\n",
        "  \n",
        "  correct = (LABEL.vocab.itos[pred_class] == input_label)\n",
        "  result = '✓' if correct else '✗'\n",
        "\n",
        "  print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]} AND song is: {input_label} {result}')\n",
        "\n",
        "  if correct:\n",
        "    corrects += 1\n",
        "\n",
        "print(f'{corrects}/10')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "input: (Hip Hop) => also feature on sway and king tech s this or that ...\n",
            "Predicted class is: 2 = Hip Hop AND song is: Hip Hop ✓\n",
            "\n",
            "input: (Rock) => we be force to live in silence eat dust and breath...\n",
            "Predicted class is: 0 = Rock AND song is: Rock ✓\n",
            "\n",
            "input: (Pop) => be you ready it s time for me to take it i be the ...\n",
            "Predicted class is: 1 = Pop AND song is: Pop ✓\n",
            "\n",
            "input: (Rock) => read some kerouac and it put me on the track to bu...\n",
            "Predicted class is: 0 = Rock AND song is: Rock ✓\n",
            "\n",
            "input: (Pop) => i come into this world without a single idea that ...\n",
            "Predicted class is: 2 = Hip Hop AND song is: Pop ✗\n",
            "\n",
            "input: (Rock) => pop drone if i ever get back home again if i ever ...\n",
            "Predicted class is: 1 = Pop AND song is: Rock ✗\n",
            "\n",
            "input: (Pop) => x4 if i can escape i would but ﻿1 of all let me sa...\n",
            "Predicted class is: 1 = Pop AND song is: Pop ✓\n",
            "\n",
            "input: (Hip Hop) => when you say it be over you shoot right through my...\n",
            "Predicted class is: 2 = Hip Hop AND song is: Hip Hop ✓\n",
            "\n",
            "input: (Pop) => imagine a world with no hate let s go if you wanna...\n",
            "Predicted class is: 1 = Pop AND song is: Pop ✓\n",
            "\n",
            "input: (Rock) => go down to taneytown i go down to taneytown to see...\n",
            "Predicted class is: 0 = Rock AND song is: Rock ✓\n",
            "8/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTnJEcmb6K-q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "cfa280d2-97eb-4f85-c03f-2a50599d62f1"
      },
      "source": [
        "genius = lyricsgenius.Genius(\"0jwZEZBbtda-jlaUnaKAR2jDleXsw5SKB6GhuTWneS1u-efwOpUYBsaTDOW185He\")\n",
        "genius.verbose = True\n",
        "\n",
        "artist = genius.search_artist('Eminem', max_songs=1, sort=\"popularity\")\n",
        "custom_title = artist.songs[0].title\n",
        "custom_text = artist.songs[0].lyrics\n",
        "custom_label = \"unk\"\n",
        "\n",
        "song_found = False\n",
        "# for td in train_data:\n",
        "#   if td.SName == custom_title.lower().strip():\n",
        "#     song_found = True\n",
        "#     break\n",
        "\n",
        "if song_found:\n",
        "  print(f'\\ncancion entrenada')\n",
        "else:\n",
        "  print(f'\\ncancion nunca antes vista')\n",
        "\n",
        "genius_songs = pd.DataFrame({ 'Lyric': [custom_text], 'Genre': [custom_label], 'Artist': [artist.name], 'SName': [custom_title] })\n",
        "\n",
        "# mismo procesamiento que el dataset utilizado\n",
        "genius_songs['Lyric'] = genius_songs['Lyric'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
        "genius_songs['Lyric'] = genius_songs['Lyric'].apply(lambda x: re.sub(r'\\[[^\\]]*\\]', '', x))\n",
        "genius_songs['Lyric'] = genius_songs['Lyric'].apply(normalize, lowercase=True, remove_stopwords=False)\n",
        "genius_songs['Len'] = genius_songs['Lyric'].apply(lambda x: len(x.split(' ')))\n",
        "\n",
        "genius_songs.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching for songs by Eminem...\n",
            "\n",
            "Song 1: \"Rap God\"\n",
            "\n",
            "Reached user-specified song limit (1).\n",
            "Done. Found 1 songs.\n",
            "\n",
            "cancion nunca antes vista\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"look i be go to go easy on you not to hurt your feeling but i be only go to get this one chance something s wrong i can feel it just a feel i have get like something s about to happen but i don t know what if that mean what i think it mean we re in trouble big trouble and if he be a banana a you say i be not take any chance you be just what the doc order i be beginnin to feel like a rap god rap god all my people from the front to the back nod back nod now who think their arm be long enough to slap box slap box they say i rap like a robot so call me rap bot but for me to rap like a computer it must be in my gene i get a laptop in my back pocket my pen will go off when i half cock it get a fat knot from that rap profit make a livin and a killin off it ever since bill clinton be still in office with monica lewinsky feelin on his nutsack i be a mc still a honest but a rude and a indecent a all hell syllable skill a holic this flippity dippity hippity hip hop you don t really wanna get into a pissin match with this rappity brat packin a mac in the back of the ac backpack rap crap yap yap yackety yack and at the exact same time i attempt this lyrical acrobat stunt while i be practicin that i will still be able to break a motherfuckin table over the back of a couple of faggot and crack it in half only realize it be ironic i be sign to aftermath after the fact how can i not blow all i do be drop f bomb feel my wrath of attack rappers be have a rough time period here s a maxi pad it s actually disastrously bad for the wack while i be masterfully construct this masterpi ce cause i be beginnin to feel like a rap god rap god all my people from the front to the back nod back nod now who think their arm be long enough to slap box slap box let me show you maintainin this shit ain t that hard that hard everybody want the key and the secret to rap immortality like have get good to be truthful the blueprint s simply rage and youthful exuberance everybody love to root for a nuisance hit the earth like a asteroid do nothing but shoot for the moon since mcs get take to school with this music cause i use it a a vehicle to bus the rhyme now i lead a new school full of student me i be a product of rakim lakim shabazz 2pac n w a cube hey doc ren yella eazy thank you they get slim inspire enough to one day grow up blow up and be in a position to meet run have be c induct them into the motherfuckin rock and roll hall of fame even though i will walk in the church and burst in a ball of flame only hall of fame i will be induct in be the alcohol of fame on the wall of shame you fag think it s all a game til i walk a flock of flame off a plank and tell me what in the fuck be you thinkin little gay lookin boy so gay i can barely say it with a straight face lookin boy you re witnessin a mass occur like you re watch a church gather take place lookin boy oy vey that boy s gay that s all they say lookin boy you get a thumb up pat on the back and a way to go from your label every day lookin boy hey lookin boy what you say lookin boy i get a hell yes from dre lookin boy i ma work for everything i have never ask nobody for shit get outta my face lookin boy basically boy you re never go to be capable of keepin up with the same pace lookin boy cause i be beginnin to feel like a rap god rap god all my people from the front to the back nod back nod the way i be racin around the track call me nascar nascar dale earnhardt of the trailer park the white trash god kneel before general zod this planet s krypton no asgard asgard so you will be thor i will be odin you rodent i be omnipotent let off then i be reloadin immediately with this bomb i be totin and i should not be wake i be the walkin dead but i be just a talkin head a zombie floatin but i get your mom deep throatin i be out my ramen noodle we have nothing in common poodle i be a doberman pinch yourself in the arm and pay homage pupil it s me my honesty s brutal but it s honestly futile if i don t utilize what i do though for good at little once in a while so i wanna make sure somewhere in this chicken scratch i scribble and doodle enough rhyme to maybe try to help get some people through tough time but i get to keep a few punchlines just in case cause even you unsigned rappers be hungry lookin at me like it s lunchtime i know there be a time where once i be king of the underground but i still rap like i be on my pharoahe monch grind so i crunch rhyme but sometimes when you combine appeal with the skin color of mine you get too big and here they come tryin to censor you like that one line i say on i be back from the mathers lp 1 when i try to say i will take seven kid from columbine put -PRON- all in a line add a ak 47 a revolver and a 9 see if i get away with it now that i ain t a big a i be but i be morphin into a immortal comin through the portal you re stick in a time warp from 2004 though and i don t know what the fuck that you rhyme for you re pointless a rapunzel with fuckin cornrows you write normal fuck be normal and i just buy a new raygun from the future just to come and shoot you like when fabolous make ray j mad cause fab say he look like a fag at mayweather s pad singin to a man while he play piano man oh man that be a 24 7 special on the cable channel so ray j go straight to the radio station the very next day hey fab i ma kill you lyric comin at you at supersonic speed uh summa lumma dooma lumma you assumin i be a human what i get to do to get it through to you i be superhuman innovative and i be make of rubber so that anything you say be ricochetin off of me and it will glue to you and i be devastate much than ever demonstrate how to give a motherfuckin audience a feel like it s levitate never fade and i know the haters be forever wait for the day that they can say i fall off they will be celebrate cause i know the way to get -PRON- motivate i make elevate music you make elevator music oh he s too mainstream good that s what they do when they get jealous they confuse it it s not hip hop it s pop cause i find a hella way to fuse it with rock shock rap with doc throw on lose yourself and make -PRON- lose it i don t know how to make song like that i don t know what word to use let me know when it occur to you while i be rippin any one of this verse that versus you it s curtain i be inadvertently hurtin you how many verse i get to murder to prove that if you be half a nice your song you can sacrifice virgin too ugh school flunky pill junkie but look at the accolade this skill brung me full of myself but still hungry i bully myself cause i make me do what i put my mind to and i be a million league above you ill when i speak in tongue but it s still tongue in cheek fuck you i be drink so satan take the fuck wheel i ma sleep in the front seat bumpin heavy have and the boyz still chunky but funky but in my head there s something i can feel tug and struggle angel fight with devil and here s what they want from me they re askin me to eliminate some of the woman hate but if you take into consideration the bitter hatred i have then you may be a little patient and much sympathetic to the situation and understand the discrimination but fuck it life s handin you lemon make lemonade then but if i can t batter the woman how the fuck be i suppose to bake them a cake then don t mistake him for satan it s a fatal mistake if you think i need to be overseas and take a vacation to trip a broad and make her fall on her face and don t be a retard be a king think not why be a king when you can be a god\",\n\"unk\",\n\"Eminem\",\n\"Rap God\",\n{\n            'v': 1633,\n            'f': \"1633\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Lyric\"], [\"string\", \"Genre\"], [\"string\", \"Artist\"], [\"string\", \"SName\"], [\"number\", \"Len\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lyric</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Artist</th>\n",
              "      <th>SName</th>\n",
              "      <th>Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>look i be go to go easy on you not to hurt you...</td>\n",
              "      <td>unk</td>\n",
              "      <td>Eminem</td>\n",
              "      <td>Rap God</td>\n",
              "      <td>1633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Lyric Genre  ...    SName   Len\n",
              "0  look i be go to go easy on you not to hurt you...   unk  ...  Rap God  1633\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJHavmfGWOWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa2a99f-7ee5-4cff-aeaf-96ca29963e76"
      },
      "source": [
        "pred_class = predict_class(model, genius_songs.iloc[0].Lyric)\n",
        "print(f'Predicted class is: {pred_class} => {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 2 => Hip Hop\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}